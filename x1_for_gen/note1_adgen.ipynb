{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c71267e",
   "metadata": {},
   "source": [
    "试一下adgen数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843c7972",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n",
      "11/12/2021 17:15:47 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "11/12/2021 17:15:47 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/adgen/2/runs/Nov12_17-15-47_192.168.1.220,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "output_dir=output/adgen/2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=50,\n",
      "per_device_train_batch_size=50,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/adgen/2,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=2000,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/vocab.txt from cache at /Users/xuehuiping/.cache/huggingface/transformers/feb7fcba07a5cd52dab8daea7c7654f9f450cf4e2586eb946df713da5b44d5e4.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/special_tokens_map.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/c7a2ad3ce29650bde9ea8929d9d4414f1472f2eaee89e1700413a60725333838.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/tokenizer_config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e8916bb2271881244e34cad9e88d11ef38394196b1d328d76773fde6934c0ef9.4930bdcbc6f75dead7cdeadc249fdb55dcb3cd75bdcee68ee5fcd8aeb6e6e359\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e0ab1af8221a3166de9abfc42b6eb4275cfe6ee6ee31a99937350dfae50cc659.b46ea2f32c0c0a3eb762ff2b81ebfe0a058025072aa60e54714633acdd9ca36e\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "loading configuration file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e0ab1af8221a3166de9abfc42b6eb4275cfe6ee6ee31a99937350dfae50cc659.b46ea2f32c0c0a3eb762ff2b81ebfe0a058025072aa60e54714633acdd9ca36e\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/pytorch_model.bin from cache at /Users/xuehuiping/.cache/huggingface/transformers/9032bba23e7bf4f5ef19608e2158df5ceb26f362b42e2b8cf5b07ea175f1c1e9.2251c3a1e4e718fc7d03740d3283002a38b4fb4b94f6f2461c49426385adc6dc\n",
      "Some weights of the model checkpoint at fnlp/bart-base-chinese were not used when initializing CPTForConditionalGeneration: ['encoder.layers.1.final_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layernorm_embedding.weight', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.fc2.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.fc2.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layernorm_embedding.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.embed_positions.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.fc2.bias', 'encoder.layers.0.fc2.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CPTForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CPTForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CPTForConditionalGeneration were not initialized from the model checkpoint at fnlp/bart-base-chinese and are newly initialized: ['encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.embeddings.position_ids', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.4.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 35.49ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 45.72ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 38.68ba/s]\n",
      "10\n",
      "The following columns in the training set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 50\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 50\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      " 20%|█████████                                    | 1/5 [00:13<00:53, 13.37s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A                                                                             {'eval_loss': 9.605729103088379, 'eval_rouge-1': 8.1576, 'eval_rouge-2': 0.1307, 'eval_rouge-l': 7.0485, 'eval_gen_len': 128.0, 'eval_runtime': 44.0306, 'eval_samples_per_second': 0.227, 'eval_steps_per_second': 0.023, 'epoch': 1.0}\n",
      " 20%|█████████                                    | 1/5 [00:57<00:53, 13.37s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.99it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:41, 20.99s/it]                                                          \u001b[A\n",
      " 40%|██████████████████                           | 2/5 [01:52<03:11, 63.68s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 9.140509605407715, 'eval_rouge-1': 7.0111, 'eval_rouge-2': 0.0, 'eval_rouge-l': 6.4966, 'eval_gen_len': 128.0, 'eval_runtime': 42.2055, 'eval_samples_per_second': 0.237, 'eval_steps_per_second': 0.024, 'epoch': 2.0}\n",
      " 40%|██████████████████                           | 2/5 [02:34<03:11, 63.68s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.11it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:42, 21.01s/it]                                                          \u001b[A\n",
      " 60%|███████████████████████████                  | 3/5 [03:28<02:37, 78.64s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.852800369262695, 'eval_rouge-1': 7.5764, 'eval_rouge-2': 0.3047, 'eval_rouge-l': 6.5667, 'eval_gen_len': 128.0, 'eval_runtime': 43.1844, 'eval_samples_per_second': 0.232, 'eval_steps_per_second': 0.023, 'epoch': 3.0}\n",
      " 60%|███████████████████████████                  | 3/5 [04:11<02:37, 78.64s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.96it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:42, 21.07s/it]                                                          \u001b[A\n",
      " 80%|████████████████████████████████████         | 4/5 [05:06<01:26, 86.01s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.701875686645508, 'eval_rouge-1': 7.8105, 'eval_rouge-2': 0.1905, 'eval_rouge-l': 6.6072, 'eval_gen_len': 128.0, 'eval_runtime': 42.1752, 'eval_samples_per_second': 0.237, 'eval_steps_per_second': 0.024, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████████         | 4/5 [05:48<01:26, 86.01s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.14it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:41, 20.92s/it]                                                          \u001b[A\n",
      "100%|█████████████████████████████████████████████| 5/5 [06:42<00:00, 89.66s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.625823020935059, 'eval_rouge-1': 8.0191, 'eval_rouge-2': 0.0995, 'eval_rouge-l': 6.4217, 'eval_gen_len': 128.0, 'eval_runtime': 42.0018, 'eval_samples_per_second': 0.238, 'eval_steps_per_second': 0.024, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████████| 5/5 [07:24<00:00, 89.66s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.89it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:42, 21.00s/it]                                                          \u001b[A\n",
      "                      \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 486.1548, 'train_samples_per_second': 0.103, 'train_steps_per_second': 0.01, 'train_loss': 9.478118896484375, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████████| 5/5 [08:06<00:00, 97.23s/it]\n",
      "Saving model checkpoint to output/adgen/2\n",
      "Configuration saved in output/adgen/2/config.json\n",
      "Model weights saved in output/adgen/2/pytorch_model.bin\n",
      "tokenizer config file saved in output/adgen/2/tokenizer_config.json\n",
      "Special tokens file saved in output/adgen/2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     9.4781\n",
      "  train_runtime            = 0:08:06.15\n",
      "  train_samples            =         10\n",
      "  train_samples_per_second =      0.103\n",
      "  train_steps_per_second   =       0.01\n",
      "The following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_gen.py --model_path fnlp/bart-base-chinese \\\n",
    " --dataset adgen --data_dir demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ddf69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "label: 这款阔腿裤，整体设计简约利落，时尚的阔腿款式带来鲜明的几何设计美感，褪去传统装束的厚重与臃肿，更具轻盈美感。搭配七分裤长修饰出挺拔的腿部线条，气质的格纹图案不显单调，尽显女性优雅气质。斜门襟设计潮流出众，让你时刻保持动人的女性风采。\n",
      "predict: 移[格案移案移移移案案案移夹移案夹移移夹案移嫌图裤案案夹案案嫌移移嫌移案绑移移绑移案嫌案移穿(图裤图裤移案,图裤穿案绑案,穿案移求案夹图裤夹移夹法移移求裤案,案法移夹图立移案(案法案案一图裤复移案法夹案立移移绊图平移案求移案图裤绑案一案立案裤案裤夹案法\n",
      "\n",
      "label: 这款充满甜美气息的针织衫，看似简单，但却充满了设计感。粗针的织法结合花纱，复古又不失时髦的感觉。饱满的颜色，散发着浓郁的浪漫气息，特别适合这个冬天过渡春天的季节，完全可以应付各种风格不同的日常穿搭。宽松的版型也让上身变得更加完美，修饰出好身材。\n",
      "predict: 松-法松松松壁松松穿案松壁穿-化,松松图针法穿,松法法版松松法针,,松针法法配配,,化配,松案松针针,化版法版配-,松化法版法针法松案穿,针针法配案松案法配,针案松法化针法壁松案配-法版壁松-松案上法针化法化得化,化针针化配-松法松-化得针法案配法松法\n",
      "\n",
      "label: 亮片钉珠v领毛衣精选优质细腻的羊毛与马海毛混纺面料，手感细腻春花，轻盈亲肤不紧绷，宽松的版型提升气质，显高显瘦。精致的大v版型加上后背的一字领修饰，彰显整体唯美气质，淑女典雅。\n",
      "predict: 壁,化穿,,化版版配版-版,,版版版加版混版:,,配版配配,,,--,,:,:化版一配,-配版得配,:配版公版配上配配配化配配:配配装配配---化配,。,化,配,化配化合公工配,配配]配,工配配工配化工化配-配配人配,合工配-工工-配-得配配合工工工配\n",
      "\n",
      "label: moss以偏绿色的基调打造的这款花色长裤，整体采用了垂坠感十足的雪纺纱质地，结合精美的大印花点缀，带来复古且知性的阔腿裤剪裁，是非常实穿的组合。设计师为这款裤子做了长款的剪裁，一直盖住脚背，充分拉长了腿部的线条感。\n",
      "predict: 案穿案案案移案案署移案移移案署图裤案案法案案求案案嫌案案复案案绑案案富案案穿移案法色案案壁案案配案案夹案案色案移法案署色移案求移案嫌复案移穿案移嫌案移署案案歪案案屑案案皮案案抬案案立案案文案案粉案案留案案图案案桌案案松案案肥案案化色案法肥案署案法穿案署\n",
      "\n",
      "label: 以落肩宽松轮廓勾勒连帽卫衣款式，经典设计尽展休闲气息，更具轻松活力。撞色的抽绳设计加入水晶贴片点缀，抢占视野，时髦个性的魅力轻松演绎，以及抓绒保暖面料，让你倍感柔软温暖。\n",
      "predict: 穿,穿配穿,,穿穿穿松法法配-,法穿松穿法穿案穿-配穿配配,,配穿穿配法案穿,配配穿:,,,法法法穿法法,配法配上法穿得配法穿,上,配,配-配,穿-,穿松合配穿法松法,,上版配-法穿配,:,法配穿松上穿穿,:配配松法配,上配松穿配案法法上穿配上,穿,法\n",
      "\n",
      "label: 来自英氏的男童休闲外套，融入经典的拼接撞色元素，带着满满的时尚气息，非常吸睛夺目，轻松把宝宝打造成街头酷感十足的小王子哦。罗纹束口的衣袖，能够自然紧密的贴合宝宝手腕，不留寒风侵入的空隙，带来温暖的穿衣体验哦。\n",
      "predict: 穿,穿法穿-,,配撞穿穿,外配配,,穿配撞裹衣案外穿,配穿配穿过,配配配-配[,配足案外配,外的]配,配,穿--配配外,配过,外足-,得穿过配,上配--穿-配,-外足,配]配]穿配,[配配法配配得配上配的得配的配,得配配足配穿穿配-穿配得穿施,上足配配\n",
      "\n",
      "label: 这件开衫的针织衫取消前门襟的扣子，增添了一丝随性。连帽款式的宽松版型有一定的厚度，非常适合春季穿着。长款的开衫针织衫，设计搭配前片贴袋装饰，实用方便，挡风保暖效果也很好。\n",
      "predict: 穿,披,披版松扣配松版扣,配扣松版配松配松上配松,松,配披,配松-松上扣配配配扣穿配,松上版松上-配扣扣配-配穿配配,配配版松配,穿松化,扣松上法扣-配披-配松化配配化,松配扣上版,夹版配披配-版配配夹配配上配配法配上版-配配松合-配夹版-夹化配-夹上\n",
      "\n",
      "label: 采用撞色的搭配在时尚搭配上具有出色的亮点，同时这款连衣裙具有复古的风格，将多种元素完美搭配。满足了女性对衣着的多方面需求同时深色与浅色碰撞的自然配色。这款带来的灵动飘逸的独特效果。袖口的花边设计耐看耐穿增加了衣身整体的立体效果。突出了品牌对于时尚的不断追求。\n",
      "predict: 穿颜颜穿撞穿案上色穿-色穿色色穿案穿穿法穿撞,穿穿色移色案穿案色穿穿案案穿移色穿颜穿案颜移色移案浅色穿法案穿色穿移撞案浅法穿案(穿穿撞撞穿色撞移色壁穿色(穿案移撞穿穿移穿穿穿嫌穿案触穿案撞案颜穿色法案案色撞案案移色撞壁穿颜移穿色案色法穿穿壁穿案法色撞穿\n",
      "\n",
      "label: 高腰直筒裤的设计效果穿上身后能够起到显瘦藏肉的作用，将双腿更好的包裹让妹子们可以轻松驾驭穿出时髦潮范的美感。腰带装饰自然垂坠，拉紧之后更有着修身作用，更为灵活的穿着效果轻松驾驭，让妹子们可以展现自身迷人的气质。\n",
      "predict: 夹,直筒筒筒][外筒]直筒-[[[配]][[]]]-[][]---]]直-]-直][-[-外-[直]--[.[-。[]:[]直]]显]-。]-要-[：[-直-[:直]:]直.[]包]]：直].]直:]-工显-][：]直直--[]以]].直]直法[]\n",
      "\n",
      "label: 这是相当性感的一款白衬衣，只做了白色一款色系，是很不挑人穿的。整个版型比较的修身的，轻松的凹显你身材曲线，衬得格外的有魅力。短款的裁剪给相当的格力，视觉上衬得你特别的高，显得你的身材比例很赞。心机的是领口的大v领，凹显你性感的锁骨曲线，还衬得你的脸型比较的小。\n",
      "predict: 穿,穿-,,穿,配配穿-配穿,外穿,,配]穿穿配穿上配穿得配穿[可穿穿格穿穿色穿穿穿-上穿穿[配穿穿外得格穿色配穿色色穿配配色穿包上配[穿穿上上穿[穿配上穿外配穿外可配穿]穿配得穿上穿-外配,穿外穿穿得,穿配公配穿配比上穿得上穿上[穿外手穿穿内[穿格配\n",
      "\n",
      "0.0021449889823111465\n"
     ]
    }
   ],
   "source": [
    "!python run_bleu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb350e3",
   "metadata": {},
   "source": [
    "aggen用blue-4评估"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
