{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7c93c7",
   "metadata": {},
   "source": [
    "自己整理了LCSTS数据集，共100个。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f0416a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n",
      "11/12/2021 19:15:21 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "11/12/2021 19:15:21 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/lcsts/1/runs/Nov12_19-15-21_192.168.1.220,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "output_dir=output/lcsts/1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=50,\n",
      "per_device_train_batch_size=50,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/lcsts/1,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=1000,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/vocab.txt from cache at /Users/xuehuiping/.cache/huggingface/transformers/feb7fcba07a5cd52dab8daea7c7654f9f450cf4e2586eb946df713da5b44d5e4.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/special_tokens_map.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/c7a2ad3ce29650bde9ea8929d9d4414f1472f2eaee89e1700413a60725333838.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/tokenizer_config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e8916bb2271881244e34cad9e88d11ef38394196b1d328d76773fde6934c0ef9.4930bdcbc6f75dead7cdeadc249fdb55dcb3cd75bdcee68ee5fcd8aeb6e6e359\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e0ab1af8221a3166de9abfc42b6eb4275cfe6ee6ee31a99937350dfae50cc659.b46ea2f32c0c0a3eb762ff2b81ebfe0a058025072aa60e54714633acdd9ca36e\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "loading configuration file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e0ab1af8221a3166de9abfc42b6eb4275cfe6ee6ee31a99937350dfae50cc659.b46ea2f32c0c0a3eb762ff2b81ebfe0a058025072aa60e54714633acdd9ca36e\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/pytorch_model.bin from cache at /Users/xuehuiping/.cache/huggingface/transformers/9032bba23e7bf4f5ef19608e2158df5ceb26f362b42e2b8cf5b07ea175f1c1e9.2251c3a1e4e718fc7d03740d3283002a38b4fb4b94f6f2461c49426385adc6dc\n",
      "Some weights of the model checkpoint at fnlp/bart-base-chinese were not used when initializing CPTForConditionalGeneration: ['encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.5.fc2.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.4.fc2.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.embed_tokens.weight', 'encoder.layers.4.fc1.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.fc1.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.3.fc2.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.embed_positions.weight', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.bias']\n",
      "- This IS expected if you are initializing CPTForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CPTForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CPTForConditionalGeneration were not initialized from the model checkpoint at fnlp/bart-base-chinese and are newly initialized: ['encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.embeddings.position_ids', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.0.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.34ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 33.09ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.58ba/s]\n",
      "10\n",
      "The following columns in the training set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 50\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 50\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      " 20%|████████▊                                   | 2/10 [01:13<04:33, 34.23s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 9.083661079406738, 'eval_rouge-1': 9.4129, 'eval_rouge-2': 0.6897, 'eval_rouge-l': 8.5038, 'eval_gen_len': 30.0, 'eval_runtime': 14.9763, 'eval_samples_per_second': 0.668, 'eval_steps_per_second': 0.067, 'epoch': 1.0}\n",
      " 20%|████████▊                                   | 2/10 [01:28<04:33, 34.23s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 21.26it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:20, 10.01s/it]                                                          \u001b[A\n",
      " 40%|█████████████████▌                          | 4/10 [03:01<04:26, 44.35s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.73902702331543, 'eval_rouge-1': 8.2645, 'eval_rouge-2': 0.0, 'eval_rouge-l': 8.2645, 'eval_gen_len': 30.0, 'eval_runtime': 16.8747, 'eval_samples_per_second': 0.593, 'eval_steps_per_second': 0.059, 'epoch': 2.0}\n",
      " 40%|█████████████████▌                          | 4/10 [03:18<04:26, 44.35s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.46it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:27, 13.67s/it]                                                          \u001b[A\n",
      " 60%|██████████████████████████▍                 | 6/10 [05:08<03:25, 51.31s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.57390022277832, 'eval_rouge-1': 9.1618, 'eval_rouge-2': 0.0, 'eval_rouge-l': 8.2922, 'eval_gen_len': 30.0, 'eval_runtime': 15.7056, 'eval_samples_per_second': 0.637, 'eval_steps_per_second': 0.064, 'epoch': 3.0}\n",
      " 60%|██████████████████████████▍                 | 6/10 [05:24<03:25, 51.31s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 31.11it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:14,  7.45s/it]                                                          \u001b[A\n",
      " 80%|███████████████████████████████████▏        | 8/10 [06:46<01:34, 47.40s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.47176456451416, 'eval_rouge-1': 8.6538, 'eval_rouge-2': 0.0, 'eval_rouge-l': 7.7842, 'eval_gen_len': 30.0, 'eval_runtime': 14.6982, 'eval_samples_per_second': 0.68, 'eval_steps_per_second': 0.068, 'epoch': 4.0}\n",
      " 80%|███████████████████████████████████▏        | 8/10 [07:01<01:34, 47.40s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 19.92it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:15,  7.51s/it]                                                          \u001b[A\n",
      "100%|███████████████████████████████████████████| 10/10 [08:26<00:00, 46.07s/it]The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 8.426956176757812, 'eval_rouge-1': 7.9033, 'eval_rouge-2': 0.0, 'eval_rouge-l': 7.0337, 'eval_gen_len': 30.0, 'eval_runtime': 20.9854, 'eval_samples_per_second': 0.477, 'eval_steps_per_second': 0.048, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████████| 10/10 [08:47<00:00, 46.07s/it]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.15it/s]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "\n",
      "2it [00:21, 10.59s/it]                                                          \u001b[A\n",
      "                      \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 548.4884, 'train_samples_per_second': 0.729, 'train_steps_per_second': 0.018, 'train_loss': 9.232759094238281, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████████| 10/10 [09:08<00:00, 54.85s/it]\n",
      "Saving model checkpoint to output/lcsts/1\n",
      "Configuration saved in output/lcsts/1/config.json\n",
      "Model weights saved in output/lcsts/1/pytorch_model.bin\n",
      "tokenizer config file saved in output/lcsts/1/tokenizer_config.json\n",
      "Special tokens file saved in output/lcsts/1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     9.2328\n",
      "  train_runtime            = 0:09:08.48\n",
      "  train_samples            =         80\n",
      "  train_samples_per_second =      0.729\n",
      "  train_steps_per_second   =      0.018\n",
      "The following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 50\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "!python run_gen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebebb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "0.004612479583652904\n"
     ]
    }
   ],
   "source": [
    "# 评估一下\n",
    "!python run_bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e147b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
