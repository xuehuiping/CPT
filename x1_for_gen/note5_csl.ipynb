{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa641b5",
   "metadata": {},
   "source": [
    "原始的test没有标签，把dev复制为test，进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d2c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n",
      "11/15/2021 10:24:31 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "11/15/2021 10:24:31 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.EPOCH,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output/csl/2/runs/Nov15_10-24-31_192.168.1.220,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=100.0,\n",
      "output_dir=output/csl/2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=50,\n",
      "per_device_train_batch_size=50,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output/csl/2,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.NO,\n",
      "save_total_limit=None,\n",
      "seed=2000,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/vocab.txt from cache at /Users/xuehuiping/.cache/huggingface/transformers/feb7fcba07a5cd52dab8daea7c7654f9f450cf4e2586eb946df713da5b44d5e4.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/special_tokens_map.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/c7a2ad3ce29650bde9ea8929d9d4414f1472f2eaee89e1700413a60725333838.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/tokenizer_config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e8916bb2271881244e34cad9e88d11ef38394196b1d328d76773fde6934c0ef9.4930bdcbc6f75dead7cdeadc249fdb55dcb3cd75bdcee68ee5fcd8aeb6e6e359\n",
      "loading file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e0ab1af8221a3166de9abfc42b6eb4275cfe6ee6ee31a99937350dfae50cc659.b46ea2f32c0c0a3eb762ff2b81ebfe0a058025072aa60e54714633acdd9ca36e\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "loading configuration file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/config.json from cache at /Users/xuehuiping/.cache/huggingface/transformers/e0ab1af8221a3166de9abfc42b6eb4275cfe6ee6ee31a99937350dfae50cc659.b46ea2f32c0c0a3eb762ff2b81ebfe0a058025072aa60e54714633acdd9ca36e\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/fnlp/bart-base-chinese/resolve/main/pytorch_model.bin from cache at /Users/xuehuiping/.cache/huggingface/transformers/9032bba23e7bf4f5ef19608e2158df5ceb26f362b42e2b8cf5b07ea175f1c1e9.2251c3a1e4e718fc7d03740d3283002a38b4fb4b94f6f2461c49426385adc6dc\n",
      "Some weights of the model checkpoint at fnlp/bart-base-chinese were not used when initializing CPTForConditionalGeneration: ['encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.3.fc1.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.fc2.weight', 'encoder.layernorm_embedding.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.k_proj.bias', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.embed_positions.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.fc2.weight', 'encoder.layers.1.self_attn.k_proj.bias', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.fc1.bias', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.self_attn.k_proj.bias', 'encoder.layers.4.fc1.bias', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.2.fc1.bias', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.k_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.3.fc2.weight', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.k_proj.bias', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.5.fc1.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layernorm_embedding.bias', 'encoder.layers.1.fc2.bias', 'encoder.embed_tokens.weight', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.fc1.weight', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CPTForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CPTForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CPTForConditionalGeneration were not initialized from the model checkpoint at fnlp/bart-base-chinese and are newly initialized: ['encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.embeddings.position_ids', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:09<00:00,  3.02s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.41s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.39s/ba]\n",
      "500\n",
      "The following columns in the training set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 50\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 50\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6000\n",
      "  0%|▏                                   | 26/6000 [46:26<179:44:36, 108.32s/it]"
     ]
    }
   ],
   "source": [
    "!python run_gen.py --dataset csl --epoch 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e02c42",
   "metadata": {},
   "source": [
    "2021-11-15 10:24:35 开始训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
