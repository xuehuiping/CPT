CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation

Yunfan Shao,1 Zhichao Geng,1 Yitao Liu,1 Junqi Dai,1 Fei Yang,2 Li Zhe,2 Hujun Bao,2 Xipeng Qiu*1

1 School of Computer Science, Fudan University
 1 Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
 2 Zhejiang Lab, Hangzhou, China {yfshao19,zcgeng20,xpqiu}@fudan.edu.cn, {yangf,lizhe}@zhejianglab.com, bao@cad.zju.edu.cn

复旦大学，邱锡鹏



![image-20211116144250759](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116144250759.png)

# Abstract

In this paper, we take the advantage of previous pre-trained models (PTMs) and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different from previous Chinese PTMs, CPT is designed for both natural language understanding (NLU) and natural language generation (NLG) tasks. CPT consists of three parts: a shared encoder, an understanding decoder, and a generation decoder. Two specific decoders with a shared encoder are pre-trained with masked language modeling (MLM) and denoising auto-encoding (DAE) tasks, respectively. With the partially shared architecture and multitask pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that fully exploits the potential of the model. Moreover, the unbalanced Transformer saves the computational and storage cost, which makes CPT competitive and greatly accelerates the inference of text generation. Experimental results on a wide range of Chinese NLU and NLG tasks show the effectiveness of CPT1.

本文提出CPT，既可以NLU又可以NLG。

CPT由3部分组成：a shared encoder、 an understanding decoder、 a generation decoder

两个解码器分别使用MLM和DAE进行预训练。

通过部分共享架构和多任务预训练，CPT可以：

（1） 使用两个解码器学习NLU或NLG任务的具体知识

（2）灵活微调，充分利用模型的潜力

此外，不平衡Transformer节省了计算和存储成本，这使得CPT具有竞争力，并大大加快了文本生成的推理速度。

# Introduction

Recently, large-scale pre-trained models (PTMs) have become backbone models for many natural language processing (NLP) tasks (Qiu et al. 2020b). However, existing PTMs are usually trained with different architectures and pre-training tasks. When applying PTMs to a downstream task, we should choose a suitable one as the backbone model according to its pre-training nature. For example, we usually select BERT or RoBERTa (Devlin et al. 2019; Liu et al. 2019) as the backbone model for natural language understanding (NLU) tasks, and BART or GPT (Lewis et al. 2020; Radford 2018) for natural language generation (NLG) tasks. With the success of PTMs in English, many works have been done to train the counterparts for Chinese (**Cui et al. 2019a; Sun et al. 2019; Wei et al. 2019; Zhang et al. 2020, 2021; Zeng et al. 2021**). However, these Chinese PTMs usually follow the settings of English PTMs, which makes these models focus on either language understanding or language generation, limiting their application to a much wider range of Chinese NLP tasks. Therefore, it is attractive to pre-train a joint model for both NLU and NLG tasks.

现有的预训练模型，是用不同的架构和任务训练的。用于下游任务的时候，需要视预训练特性，选择合适的骨干模型。

例如，一般使用BERT或者RoBERTa来作为NLU任务的主干模型，用BART或者GPT来处理NLG。

在英文预训练的模型基础上， 出现了很多中文预训练模型。但是这些模型延续了英文模型的架构，用起来的时候不方便，或者是NLU或者是NLG，这限制了模型在中文NLP的应用。

因此，为NLU和NLG任务预训练一个联合模型，是很有吸引力的。



Few works attempt to fuse NLU and NLG into a unified model. **UniLMs** (Dong et al. 2019; Bao et al. 2020) and **GLM** (Du et al. 2021) adapt a unified Transformer encoder for both understanding and generation; however, their architectures restrict them to employ more flexible pre-training tasks, such as denoising auto-encoding (**DAE**) used in **BART**, a widely successful pre-training task for NLG. **PALM** (Bi et al. 2020) adopts the standard Transformer and adds an auxiliary masked language modeling (**MLM**) task to enhance the understanding ability; however, it still focuses on language generation tasks.

In this paper, we propose **CPT**, a novel Chinese Pretrained Unbalanced Transformer for both NLU and NLG tasks. The architecture of CPT is very concise (as shown in Figure 1), which divides a full Transformer encoder-decoder into three parts: 1) a shared encoder to capture the common representation; 2) a decoder for understanding, which uses full self-attention and is pre-trained with masked language modeling (MLM); 3) a decoder for generation, which adopts masked self-attention and is pre-trained with the DAE task. By multi-task pre-training, CPT is able to improve the performance on both language understanding and generation, respectively.

3部分：
一个共享编码器，捕捉通用的表示；

解码器用于理解，采用了全部自注意力，用MLM预训练；

解码器用于生成，采用掩码自注意力，用DAE预训练。





![image-20211114170216121](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211114170216121.png)





The main properties of CPT are as follows:

1. CPT can be regarded as **two separated** PTMs with **a shared encoder**. Two specific decoders are pre-trained with MLM and DAE tasks, respectively. Each decoder can learn the specific knowledge on either NLU or NLG tasks, while the shared encoder learns the common knowledge for universal language representation.
2. Two separated decoders enable CPT to adapt to various downstream tasks **flexibly**. For example, CPT could be fine-tuned with at least five modes for classification tasks (as shown in Figure 2), which exploits the full potential of CPT. Thus, we could choose a suitable fine-tuning mode based on the attributes and characteristics of downstream tasks.
3. The overall architecture of CPT is an **unbalance** Transformer. To make the computational cost and the size of CPT comparable with popular PTMs, such as BERT and BART, we use a novel architecture consisting of a deeper shared encoder and two shallower decoders. Especially, the shallow generation decoder greatly accelerates the inference of text generation.

We conduct experiments on various language understanding and text generation tasks, including datasets for text classification, sequence labeling, machine reading comprehension, summarization, data-to-text generation, etc. Results show that CPT could achieve competitive results with state-of-the-art on these datasets.

# Related Work 

## PTMs towards both NLU and NLG

Recently, there are some efforts to combine language understanding and generation into a single pre-trained model. **UniLM** (Dong et al. 2019) pre-trained with an ensemble of attention masks, which allows the model to be used for both generative and classification tasks. A difference is that all parameters of UniLM are shared between generation and discrimination, whereas CPT uses two separated decoders. Thus, CPT can utilize the DAE pre-training task which is proven to be effective for NLG tasks (Lewis et al. 2020).

NLU和NLG二合一的模型；

UniLM在生成和识别时候，共享参数；但是CPT用了2个独立的解码器。

**PALM** (Bi et al. 2020) is a pre-trained model focusing on **conditional generation**. To force the encoder to comprehend the meaning of the given context, **MLM** is added to pre-train the encoder. In contrast, CPT has an individual decoder for MLM which can avoid the negative effects brought by DAE. Therefore CPT also has good performance on NLU tasks.

PALM条件生成，编码器用了MLM。

CPT为MLM引入了独立的解码器，避免了DAE（Denoising Auto-Encoding）的副作用。所以，CPT在NLU任务表现好。

More recently, **ERNIE 3.0** (Sun et al. 2021b) also uses a universal encoder and several task-specific decoders, but it adopts **Transformer-XL** as the backbone and its generative pre-training task is left-to-right LM with a special masked attention matrix. Different from ERNIE 3.0, CPT adopts the encoder-decoder architecture and is more suitable for sequence-to-sequence (Seq2Seq) tasks.

ERNIE3.0采用Transformer-XL作为主干网络，生成的预训练任务是从左到右的、掩码注意力的语言模型。CPT是编码器解码器架构，更适合seq2seq任务。

## Chinese PTMs

Many attempts have been conducted to pre-train the Chinese counterparts of PTMs.

The first line of works follows BERT and uses MLM with whole word masking strategy to pre-train Transformer encoder, such as Chinese versions of BERT and **RoBERTa** (Cui et al. 2019a), **NEZHA** (Wei et al. 2019), **ZEN** (Diao et al. 2020). Some of them add special features of Chinese characters or words to further boost the performance of NLU tasks, such as **ERNIE 1.0/2.0** (Sun et al. 2019, 2020), **ChineseBERT** (Sun et al. 2021c). However, these PTMs could not be adopted to text generation directly.

一线，追随BERT和MLM。

中文预训练模型：RoBERTa、NEZHA、ZEN、ChineseBERT、ERNIE 1.0/2.0/3.0

The second line of works follows GPT and uses the left- to-right LM task to pre-train a Transformer decoder, such as **CPM** (Zhang et al. 2021) and **PanGu** (Zeng et al. 2021). Although large-scale PTMs with tens of billions parameters have been released recently, the huge computation and storage cost hinders their applications.

二线，追随GPT、从左到右的LM。

例如：CPM、PanGu。

计算量大，存储成本高，应用难。

The third line of works aims to pre-train the full Transformer encoder-decoder. **CPM-2** (Zhang et al. 2021) follows **T5** (Raffel et al. 2020) and adopts a **Seq2Seq MLM** pretraining task, which predicts the masked tokens in a Seq2Seq fashion. Although **BART** (Lewis et al. 2020) has achieved widely success on conditional text generation tasks, such as text summarization (Dou et al. 2021; Liu and Liu 2021) and dialogue system (Lin et al. 2020), it still lacks corresponding Chinese versions2.

三线：全Transformer编码器解码器

例如：CPM-2跟随T5，采用Seq2Seq语言模型预训练；

BART在文本生成，如文本摘要、对话系统方面取得较大成功，但是还没有对应的中文版本。

Different from the above Chinese PTMs, CPT is a pretrained unbalanced Transformer with MLM and DAE tasks, which is capable of achieving competitive results on both NLU and NLG tasks. Besides, CPT is parameter efficient compared to these large-scale models. Table 1 compares different Chinese PTMs.

CPT和以上不同，采用不平衡Transformer，用MLM和DAE任务，在NLU和NLG都取得了具有竞争力的结果。此外，CPT的参数效率高。

![image-20211116145541395](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116145541395.png)

# Model Architecture

As shown in Figure 1, The architecture of CPT is a variant of the full Transformer and consists of three parts:

1. Shared Encoder (S-Enc): a Transformer encoder with fully-connected self-attention, which is designed to capture the common semantic representation for both language understanding and generation.
2. Understanding Decoder (U-Dec): a shallow Transformer encoder with fully-connected self-attention, which is designed for NLU tasks. The input of U-Dec is the output of S-Enc.
3. Generation Decoder (G-Dec): a Transformer decoder with masked self-attention, which is designed for generation tasks with **auto-regressive** fashion. G-Dec utilizes the output of S-Enc with cross-attention.

![image-20211116145605463](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116145605463.png)

CPT的模型架构见图1，是全序列Transformer，由3部分组成。

1. 共享编码器，S-Enc，Transformer编码器，全连接自注意力，捕捉语言理解和生成的共同的语义表示。
2. 理解解码器，U-Dec，浅Transformer编码器，全连接自注意力，用于NLU任务。
3. 生成解码器，G-Dec，Transformer解码器，全连接自注意力，用于自回归方式的NLG任务。G-Dec利用S-Enc的输出进行交叉注意。

With the two specific decoders, CPT can be used flexibly. For example, CPT can be easily fine-tuned for NLU tasks using just S-Enc and U-Dec, and can be regarded as the standard Transformer encoder; while for NLG tasks, CPT adopts S-Enc and G-dec, and forms a Transformer encoder-decoder. With different combinations, CPT is able to be effectively applied on various downstream tasks, which fully exploits the pre-trained parameters and obtains competitive performance. More combinations and use cases will be discussed in Fine-Tuning Section.

两个特殊的解码器，使得CPT更加灵活。

可自由组合：

S-Enc 和 U-Dec，可以看做是标准的Transformer编码器，用于NLU；

S-Enc 和 G-dec，可以形成Transformer编码器解码器，用于NLG。

不同组合，可有效用于下游不同任务。具体见微调部分。

Different from most PTMs with encoder-decoders, we exploit a **deep-shallow framework** for shared encoder and decoders. More specifically, we use a deeper encoder and two shallow decoders for CPT. We assume that a shallow decoder retains the performance on text generation and reduces decoding time, which has proven to be effective for neural machine translation (Kasai et al. 2021) and spell checking (Sun et al. 2021a).

一个深一点儿的编码器，两个浅的解码器。

The deep-shallow setup makes CPT more general for both understanding and generative tasks with minor parameter overheads. It also accelerates the inference of CPT for text generation as the G-Dec is a light decoder.



# Pre-Training

To make CPT good at both NLU and NLG tasks, we introduce two pre-training tasks.

1. Masked Language Modeling (**MLM**): We pre-train the parameters of S-Enc and U-Dec with MLM (Devlin et al.2019; Cui et al. 2019a). Given a sentence, we randomly replace some tokens with the [MASK] token and train S-Enc and U-Dec to predict the masked tokens. Following Cui et al. (2019a), we adopt Whole Word Masking (WWM) to replace the tokens. Compared to randomly token masking, WWM is more suitable for inducing semantic information carried by words and spans.
2. Denoising Auto-Encoding (**DAE**): We pre-train the parameters of S-Enc and G-Dec by reconstructing the original document based on the corrupted input. According to the studies of **BART** (Lewis et al. 2020), we corrupted the input by two effective ways. 1) *Token Infilling*: a Whole Word Masking (WWM) strategy with single mask replacement. First, a number of words are sampled based on the segmentation. Then, each selected word is replaced with a single [MASK] token, regardless of how many tokens it consists; and 2) *Sentence Permutation*: sentences are extracted from a document based on punctuation, and shuffled in a random order.

介绍2个预训练任务：

MLM：掩码语言模型。用MLM预训练S-Enc和 U-Dec的参数。依照Whole Word Masking (WWM) 替换，因为它更适合单词和片段携带的语义信息。

DAE：去噪自编码。重构原文档的输入，预训练S-Enc 和 G-Dec的参数。依照BART，2种方式打断输入：Token填充（先采样再替换）、句子重排。

In practice, We first use a Chinese Word Segmentation (**CWS**) tool to split the sentences into words. Then, we select **15%** of the words and mask the corresponding characters. For the masked characters, we follow the setup of BERT to (1) replace 80% of them with a special [MASK] token, (2) replace 10% of them by random tokens, (3) keep the rest 10% of them unchanged.

先用CWS分词，取15%单词掩盖。

比例同BERT：80%换为[MASK] 、10%替换为随机，剩余10%不变。

Finally, we train CPT with two pre-training tasks under a multi-task learning framework. Thus, CPT can learn for both understanding and generation, and can easily deal with downstream NLU or NLG tasks.

最后，在多任务学习框架下，训练CPT。可轻松处理下游任务NLU、NLG。

# Fine-Tuning

PTMs are usually fine-tuned in only few ways for a given downstream task. For example, for sentence-level classification, we fine-tune BERT by taking the top-layer output of [CLS] token as the representation of the whole sentence, while fine-tune GPT by using the representation of the last token of the sequence.

对于给定的下游任务，PTMs通常仅以几种方式进行微调。例如，对于句子级分类，我们通过将[CLS]标记的顶层输出，作为整个句子的表示，来微调BERT，而通过使用序列的最后一个标记的表示来微调GPT。

Thanks to the separated understanding and generation decoders, CPT can be fine-tuned in multiple patterns. For a given downstream task, one could choose the most suitable way to fully stimulate the potential of CPT to achieve competitive results.

正是由于独立的理解和生成解码器，CPT可以多模式微调。

对于给定的下游任务，可以选择最合适的方式来充分激发CPT的潜力，以实现竞争结果。



## Fine-Tuning for Sentence-Level Classification

When incorporating external classifiers, CPT have three finetuning modes for sequence-level classification (As shown in Figure 2 (a), (b) and (c)).

当合并外部分类器时，CPT有三种用于序列级分类的微调模式。

1. $$CPT_{u}$$: a **BERT-style** mode. The sentence representation is from U-Dec module only, which is usually the first state of` [CLS]` token.

2. $CPT_{g}$ : a **BART-style** mode. The same input is fed into the S-Enc and G-Dec, and the representation from the final output token `[SEP]` from G-Dec is used.

3. $CPT_{ug}$ : The same input is fed into the S-Enc and G-Dec, and the final representation is the concatenation of the first output of U-Dec and the final output of G-Dec.

![image-20211116152051713](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116152051713.png)

用于分类，3种微调模式：

1. BERT方式。句子表示只从U-Dec出来，一般是[CLS]的第一个状态
2. BART方式。相同的输入给S-Enc 和 G-Dec，使用来自G-Dec的最终输出标记“[SEP]”的表示。
3. 相同的输入给S-Enc 和 G-Dec，最终表示是U-Dec的第一个输出和G-Dec的最终输出的连接。

Recently, a **powerful** and **attractive** framework, **prompt based learning** (Schick and Schu ̈tze 2021; Gao, Fisch, and Chen 2021; Liu et al. 2021), is also able to boost the performance of PTMs. By **defining prompting templates** and **reformulating the classification tasks into a generative fashion**, the framework utilizes PTMs to generate words corresponding to task labels. The generative patterns are so close to the pretraining tasks of PTMs that they have the ability of few-shot or even zero-shot learning.

最近，强大而有吸引力的框架，基于提升的学习，也能够提高PTM的性能。

定义提示模板，并将**分类**任务重新格式化为**生成**方式，该框架利用PTM生成与任务标签对应的单词。

生成模式与PTMs的训练前任务非常接近，因此它们具有少量甚至零次学习的能力。

The prompt-based methods could also be applied on CPT with more flexibly fashions since CPT has two decoders. As shown in Figure 2(d) and (e), we construct prompts and convert the task into an generation task with CPT by the following two modes:

基于提示的方法也可以更灵活地应用于CPT，因为CPT有两个解码器。

我们通过以下两种模式，构造提示，用CPT将任务转换为生成任务：

![image-20211116152801769](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116152801769.png)

1. $CPT_{u+p}$: A **MLM** task. We manually construct an input template and assign a word to each task label. CPT is finetuned to predict the word at the masked positions, which will be mapped to the task labels. Since a word may be tokenized into multiple character tokens, the predicted distributions at masked positions are **averaged** to get the predicted distribution of labels.
2. $CPT_{g+p}$: **Conditional text generation**. We encode the input text with S-Enc and train CPT to generate prompt text initialized with corresponding labels by teacher forcing. For inference, we first construct the prompt text for each label. Then, the perplexity of each prompt text is calculated. Finally, the prediction is assign to the label with the highest corresponding perplexity.

1. MLM任务。手动构建输入模板，每个任务标签分配一个单词。CPT被微调，去训练掩码位置的单词，该位置将映射到任务标签。单词可被分为多个字符，所以取位置的平均值，得到预测标签的分布。

2. 条件文本生成。用S-Enc将输入文本编码，通过教师强制训练CPT生成对应标签的提示文本。为了进行推断，我们首先为每个标签构造提示文本。然后，计算每个提示文本的复杂度。最后，将预测分配给对应困惑度最高的标签。

## Fine-Tuning for Sequence Labeling

For sequence labeling, each token needs a representation for token-level classification. Similar to sequence-level classification, we leverage PTMs to obtain high quality token representations and then put the representations to a trainable classifier to assign labels for these tokens. Thus, similar to sentence-level classification, we can fine-tune CPT for sequence labeling as CPTu , CPTg and CPTug , using (1) U-Dec only, (2) G-Dec only, or (3) both U-Dec and G-Dec. Figure 3 shows two examples for sequence labeling.

序列标注，每个token需要一个token级别的分类。

和序列级别的分类一样，我们利用PTM获得高质量的令牌表示，然后将表示放入可训练分类器，为这些令牌分配标签。

因此，与句子级分类类似，我们可以使用对序列标记的CPT进行微调，如CPTu、CPTg和CPTug。

（1）单独U-Dec、（2）单独G-Dec（3）U-Dec和G-Dec都有。

![image-20211116153754776](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116153754776.png)

## Fine-Tuning for Machine Reading Comprehension

Machine Reading Comprehension requires the model to predict an **answer span** shown in the **passage** for a given **question**. A typical fine-tuning pattern is to train PTMs to predict the start and end positions of the span in the passage. The prediction is based on the tokens of the passage. Thus, CPTu, CPTg and CPTug can be fine-tuned, similar to sequence-labeling. Figure 4 shows the example of CPTu.

机器阅读理解给定一个【问题】，要求模型预测一个在【段落】中出现的【答案片段】，

一种经典的预训练模式，是训练预训练模型，预测段落中片段的开始和结束位置。

预测是基于片段中的token。

因此，CPTu, CPTg and CPTug 可以和序列标注类似的微调。

![image-20211116162637372](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116162637372.png)

 

## Fine-Tuning for Conditional Generation

Apart from NLU tasks, CPT can do text generation efficiently. As shown in Figure 5, we simply fine-tune CPTg with **S-Enc** and **G-Dec** modules on text generation tasks, similar to the usage of other auto-regressive PTMs (Lewis et al. 2020).

和其他自回归预训练模型一样，S-Enc和G-Dec模块微调CPT。

![image-20211114203612541](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211114203612541.png)

# Experiments 

## Pre-Training Setups

We implement two versions of CPT, namely, *base* and *large*, respectively consisting of 14/28 Transformer layers with 10/20 layers for shared encoder and 2/4 layers for each task specific decoder. And the hidden units and attention heads per layer for base and large versions are 768/1,024 and 12/16, respectively. The total number of layers activated for a given task is always equal to 12/24, which makes our model comparable with base/large-size of BERT and its variants (RoBERTa, ERNIE 1.0/2.0, etc).

本文实现了2个版本的CPT：base和large。

|                 | base | large |
| --------------- | ---- | ----- |
| Transformer层数 | 14   | 28    |
| 共享编码器      | 10   | 20    |
| 解码器-U 层数   | 2    | 4     |
| 解码器-G 层数   | 2    | 4     |
| 隐层单元        | 768  | 1024  |
| 注意力头数      | 12   | 16    |

We train our models on the open source large-scale raw text, **Chinese Wikipedia** and a part of **WuDaoCorpus**. The training data contains **200GB** cleaned text ranges from different domains. We use **Jieba** to segment Chinese words for Whole Word Masking and use WordPiece tokenizer inherited from BERT to split input text into tokens. We use **Adam** to train the models for **500k** steps, with the batch size of **2048**, the learning rate of **1e-4**, β1 = 0.9, β2 = 0.98, weight decay of 0.01. We warmup the learning rate for first 10,000 steps then do linear decay. In addition, a **Chinese BART** is pre-trained with the same corpora, tokenization and hyperparameters as a baseline.

训练模型，在开源大规模文本：Chinese Wikipedia和一部分WuDaoCorpus，不同领域的200GB已清洗文本。

Jieba做中文分词，用WordPiece将输入文本切分为token。



前10000步，学习率不变，之后线性递减。

另外，还用相同的语料训练了Chinese BART。

## Evaluation Tasks

To evaluate the effectiveness of our model, we conduct experiments on various NLP datasets across different understanding and generation tasks, with details illustrated below.

在跨越NLU和NLG的不同NLP的数据集做评估。

### Classification 

We evaluate the model on the Chinese Language Understanding Evaluation Benchmark (**CLUE**) (Xu et al. 2020), which contains **text classification** TNEWS, IFLYTEK, **natural language inference** (NLI), OCNLI, **sentence pair matching** (SPM) AFQMC, and **coreference resolution** (CoRE) CLUEWSC 2020 (WSC.) **key word recognition** (KwRE) CSL. We conduct **data augmentation** CSL as Zhang and Li (2020) performed, and evaluate TNEWS on version 1.1 test set. **Accuracy** is used for these datasets.

分类：在CLUE，包括文本分类、自然语言推理、句子对儿匹配、共指消岐、关键词识别

这里用TNEWS的1.1版本

CSL做了数据增强

评估指标：Accuracy

### Sequence Labeling 

We evaluate our model on Chinese word segmentation (CWS) and named entity recognition (NER), which are two representative sequence labeling tasks. We use two datasets from **SIGHAN2005** (Emerson 2005) for CWS, which are **MSR**, **PKU**. And for NER, **MSRA** (Levow 2006), **OntoNotes3** are used. We use the same dataset preprocessing and split methods as in previous work (Li et al. 2021, 2020; Qiu et al. 2020a). And **F1** scores are reported.

序列标注任务：中文分词CWS、命名实体识别NER

CWS：从SIGHAN2005里面选择2个，MSR、****

NER：MSRA、OntoNotes3

评估指标：F1

### MRC 

Span based machine reading comprehension (MRC) dataset **CMRC** 2018 (CMRC) (Cui et al. 2019b) and Traditional Chinese MRC dataset **DRCD** (Shao et al. 2018) are used. We follow the data processing in Cui et al. (2019a, 2020) and transform the text from DRCD is transformed to Simplified Chinese. The **Exact Match (EM)** scores are reported.

基于片段的MRC。

数据集：CMRC、DRCD

DRCD转为简体中文。

评估指标：EM得分

### Text Generation 

We use two abstractive summarization datasets, LCSTS (Hu, Chen, and Zhu 2015) and CSL4, and a data-to-text generation dataset, ADGEN (Shao et al. 2019) to evaluate the text generation ability of our model. Among them, LCSTS is a large corpus of Chinese short text summarization dataset constructed from Sina Weibo, consisting of 2 million real Chinese short texts with short summaries. And CSL is an academic domain text summarization dataset, constructed from abstract and titles from publications in computer science domain. And ADGEN is a data-to-text dataset that requires models to generate long text for advertisement based on some keywords. And we evaluate PTMs on test sets of LCSTS and ADGEN and the development set of CSL. The character-level Rouge-L is used to evaluate the summarization results. For ADGEN, we follow Zhang et al. (2021) to use BLEU-4.

2个摘要数据集，LCSTS和CSL，1个数据到文本生成数据集ADGEN

CSL是学术摘要数据集，计算机科学领域的出版物，标题和摘要。

ADGEN基于关键词生成长文本，用于广告。

- CSL样例：

```
{"id": 3, "title": "人脸遮挡区域检测与重建", "abst": "提出一种基于模糊主分量分析技术(FPCA)的人脸遮挡检测与去除方法.首先,有遮挡人脸被投影到特征脸空间并通过特征脸的线性组合得到一个重建人脸.计算重建图与原图的差图像,加权滤波后并归一化作为被遮挡的概率,以此概率为权重由原图和重建图合成新的人脸.在后续迭代中,根据遮挡概率使用模糊主分量分析进行分析重建,并使用累积误差进行遮挡检测.实验结果表明,算法可精确定位人脸遮挡区域,得到平滑自然的重建人脸图像,优于经典的迭代PCA方法."}
```

- LCSTS样例

![image-20211116163525752](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211116163525752.png)

- ADGEN样例

  ```
  {"feature": [["类型", "裤"], ["版型", "宽松"], ["风格", "性感"], ["图案", "线条"], ["裤型", "阔腿裤"]], "title": "", "largeSrc": "http://gw.alicdn.com/imgextra/i1/697022431/TB2G92uehWYBuNjy1zkXXXGGpXa_!!697022431-0-daren.jpg_790x10000Q75.jpg", "refSrc": "https://market.m.taobao.com/apps/market/content/index.html?&contentId=200569599939", "desc": "宽松 的 阔腿裤 这 两年 真的 吸粉 不少 ， 明星 时尚 达人 的 心头 爱 。 毕竟 好 穿 时尚 ， 谁 都 能 穿 出腿长 2 米 的 效果 宽松 的 裤腿 ， 当然 是 遮肉 小 能手 啊 。 上身 随性 自然 不 拘束 ， 面料 亲肤 舒适 贴身 体验 感 棒棒 哒 。 系带 部分 增加 设计 看点 ， 还 让 单品 的 设计 感更强 。 腿部 线条 若隐若现 的 ， 性感 撩人 。 颜色 敲 温柔 的 ， 与 裤子 本身 所 呈现 的 风格 有点 反差 萌 。", "file": "4e9cf852962fec5119cdcffa5d9c1293.jpg", "专有属性": [["裤型", "阔腿裤"], ["类型", "裤"]], "共有属性": [["版型", "宽松"], ["风格", "性感"], ["图案", "线条"]], "segment": {"seg_0": {"segId": 0, "key_type": ["裤型", "版型"], "order": [["版型", "宽松"], ["裤型", "阔腿裤"]], "seg": "宽松 的 阔腿裤 这 两年 真的 吸粉 不少 ， 明星 时尚 达人 的 心头 爱 。"}, "seg_1": {"segId": 1, "key_type": ["裤型", "版型"], "order": [["版型", "宽松"]], "seg": "毕竟 好 穿 时尚 ， 谁 都 能 穿 出腿长 2 米 的 效果 宽松 的 裤腿 ， 当然 是 遮肉 小 能手 啊 。"}, "seg_2": {"segId": 2, "key_type": ["材质"], "order": [], "seg": "上身 随性 自然 不 拘束 ， 面料 亲肤 舒适 贴身 体验 感 棒棒 哒 。"}, "seg_3": {"segId": 3, "key_type": ["<GENERAL>"], "order": [], "seg": "系带 部分 增加 设计 看点 ， 还 让 单品 的 设计 感更强 。"}, "seg_4": {"segId": 4, "key_type": ["图案", "风格"], "order": [["图案", "线条"], ["风格", "性感"]], "seg": "腿部 线条 若隐若现 的 ， 性感 撩人 。"}, "seg_5": {"segId": 5, "key_type": ["颜色", "风格"], "order": [], "seg": "颜色 敲 温柔 的 ， 与 裤子 本身 所 呈现 的 风格 有点 反差 萌 。"}}}
  ```

![](http://gw.alicdn.com/imgextra/i1/697022431/TB2G92uehWYBuNjy1zkXXXGGpXa_!!697022431-0-daren.jpg_790x10000Q75.jpg)

原始页面：https://market.m.taobao.com/apps/market/content/index.html?&contentId=200569599939



文本生成的评估指标：

字符级的Rouge-L用于评估摘要任务：LCSTS和CSL

BLEU-4用于评估ADGEN

## Compared PTMs

We compare CPT with a series of state-of-the-art PTMs for either natural language understanding or text generation. The details are as follows.

### PTMs for NLU 

PTMs with the Transformer Encoder structure and pre-trained with MLM usually perform well in NLU tasks, such as the Chinese versions of BERT and RoBERTa (Cui et al. 2019a), NEZHA (Wei et al. 2019), ERNIE 2.0 (Sun et al. 2020), MacBERT (Cui et al. 2020). Unless otherwise specified, we use BERT and RoBERTa to refer to BERT-wwm-ext and RoBERTa-wwm-ext, respectively.

预训练模型：中文BERT、RoBERTa、NEZHA、ERNIE2.0、MacBERT在NLU表现都不错。

本文如无特殊说明，BERT指BERT-wwm-ext ，RoBERTa指RoBERTa-wwm-ext

### PTMs for NLG 

For text generation, we compare CPT with generative Transformers ranging from normal size to large scale, including BART (Lewis et al. 2020), mT5 (Xue et al. 2021), CPM-2 (Zhang et al. 2021), and models with pretrained encoders. BART is a sequence-to-sequence model pretrained with DAE task. Due to the missing of Chinese version, we train a Chinese BART as the baseline on the same corpus and tokenization of CPT. mT5 is a multilingual variant of T5 pre-trained on over 101 languages, including Chinese. CPM2 is a large-scale encoder-decoder model with 11 billion parameters, pre-trained in multiple stages with large-scale Chinese and bilingual data. We also report generative models adopted from Transformer encoders such as RoBERTa and ERNIE 2.0, to further evaluate the effectiveness generative pre-training.

对于文本生成，我们把CPT和生成式Transformers进行比较，有正常尺寸，也有大规模的。

包括：BART、mT5、CPM-2、和带有预训练编码器的模型。

BART是一个序列到序列的模型，用DAE任务进行预训练。由于缺少其中文版本，我们训练了一个中文BART， Chinese BART，作为基准。用的语料、分词器，均和CPT一样。

mT5是T5的多语言版本，在101种语言进行预训练，包括中文。

CPM2是一个大规模的编码器解码器模型，有110亿参数，使用大规模中文和双语数据，进行多阶段预训练。

我们还报告了采用Transformer编码器（如**RoBERTa**和**ERNIE 2.0**）的生成模型，以进一步评估生成预训练的有效性。

## Main Results

To fully release the potential of our model, we fine-tune CPT for NLU tasks in different ways as mentioned in Fine-Tuning Section, denoted as CPTu, CPTg and CPTug, CPTu+p and CPTg+p, respectively. We use (B) and (L) to distinguish base and large version of PTMs, respectively.

### Classification 

Table 2 shows the development set results of CLUE Benchmark of different fine-tuning modes. As a result, CPTu (B) achieves a 74.6 on average, surpassing other baselines and fine-tuning patterns on base version of CPT. Besides, CPTug (L) obtains a averaged accuracy 76.2, which is better than RoBERTa (L) by a large margin. Therefore, we choose CPTu (B) and CPTug (L) as the most suitable fine-tuning patterns to do the classification. And we evaluate them on the test sets, reported in Table 3.

For prompt-based fine-tuning (Table 2), we find that directly fine-tuning without prompt works well on some datasets, with the small gaps between CPTu, CPTg and CPTug. Moreover, CPTu+p achieves good results on some datasets that even outperform methods without prompt tuning. However, the accuracy of prompt-base methods on other datasets drops a lot. As there are many factors that affect prompt tuning performance including prompt design, choices of words for labels, etc. Manually designed prompts may be suboptimal. Besides, we find that CPTg+p degenerates obviously on TNEWS and IFLYTEK. Both datasets have more than 3 classes, which contains 15 and 112 labels, respectively. Moreover, these labels are hard to represented by a single character. In practice we assign words with up to 7 characters to a label. We presume that the large number of labels and the multi-token issue hinders CPTg+p to generate correctly.

Table 3 reports the performance of CPT on classification tasks and the comparison with previous representative Chinese PTMs. We report accuracy on the test sets of these datasets. Among the fine-tuned CPTs, we choose base version CPTu and large version CPTug as they obtain the best results on development sets. Base size CPT consistently outperforms BERT, RoBERTa and ERNIE. Moreover, large size CPT achieves a 74.5 averaged score, outperforming RoBERTa (L) with a large margin.

### Sequence Labeling 

The CPT is fine-tuned as CPTu, CPTg and CPTug and evaluated on development sets. We find that CPTu constantly obtains the best development results. We conjecture that CWS and NER have more dependency on local syntax than complex semantics used for text generation. Thus, CPTu is more suitable for CWS and NER with its bidirectional fully connected self-attention. As a result, we report the test set results of CPTu to compare with other PTMs.



We compare our model with other state-of-the-art methods on sequence labeling datasets. As shown in Table 4, CPTu (L) achieves the highest performance and exceed the BERT (L), RoBERTa (L) and ERNIE (L) on all sequence labeling tasks, both CWS and NER. And CPTu (B) obtains a comparable results, surpassing base versions of BERT and RoBERTa.

### MRC 

Table 5 shows the experimental results on MRC tasks, which also indicates the effectiveness of CPT. We report the Exact Match (EM) score on CMRC dev set, DRCD dev and test sets. We try and evaluate CPTu, CPTu and CPTu on the development sets of these datasets and choose the pattern that acquires the best results to report. As a conclusion, CPTu obtains comparable or higher results compared to previous systems that are widely used, such as RoBERTa, MacBERT, ERNIE and NEZHA. Moreover, CPTu consistently outperforms other strong baselines by a large margin, with 72.3 EM score on the CMRC development set and 91.1 EM on the DRCD test set.

### Text Generation 

Table 6 compares the performance of our model on generation datasets with other strong methods. The character-level Rouge-L is used to evaluate the summarization results. For ADGEN, we follow Zhang et al. (2021) to use BLEU-4.

![image-20211115084807154](/Users/xuehuiping/Library/Application Support/typora-user-images/image-20211115084807154.png)



As a conclusion, CPTg achieves competitive performance on text generation compared with other methods, such as mT5, CPM-2, BART. In addition, compared with other pretrained encoders (RoBERTa and ERNIE 2.0), CPTg improves the generation score with the NLG enhanced pre-training. When compared with pre-trained mT5 and CPM-2, CPTg acquires better results on both base and large versions. We assume the difference of pre-training tasks that lead to the performance gaps. Both mT5 and CPM-2 exploit a T5 style masked span generation as their pre-training task, while CPT is pre-trained with DAE, which shows the effectiveness of DAE for text generation pre-training. In addition, while BART (L) have a sightly better results on CSL and ADGEN, CPTg and BART have similar results on text generation. The shallow decoder of CPTg may affect the performance on long text generation. However, the performance gaps are still small. And we believe the pre-training of the shallow decoder closes the gaps.

Moreover, because of the shallow decoder, CPT could generate texts more efficiently (Figure 6), which could be faster than other depth symmetric encoder-decoder Transformers with the same number of layers of the encoder and the decoder. As BART and CPT have similar number of parameters in both base and large versions. On all generation dataset, the decoding speed of CPT surpass BART with a large margin. Our model achieves 1.4× ∼ 1.5× speedup compared with BART and still maintain comparable generation results in base size. And CPT (L) has up to 1.7× relative speedup compared to BART (L). As a conclusion, the shallow G-Dec is able to speed up the generation with minor performance loss.

# Conclusion

In this paper, we propose CPT, a novel Chinese PTM for both language understanding and generation. With the flexible design, CPT can be assembled and disassembled in various fashions, which could fully exploit the potential of CPT. Experimental results on a wide range of Chinese NLU and NLG tasks show the effectiveness of CPT.

In future work, we will introduce more specific designs according to Chinese properties, such as better tokenization, pre-training tasks and model architectures.

